"""
CSCC11 - Introduction to Machine Learning, Winter 2022, Assignment 4 - Clustering
B. Chan, S. Wei, D. Fleet
"""

%%%%%%%%%%
%%  Step 0
%%%%%%%%%%

1) What is the average sparsity of input vectors? (in [0, 1])
   0.9866230211716414

2) Find the 10 most common terms, find the 10 least common terms.  (list, separated by commas)
   most: [year, peopl, on, game, time, first, govern, go, world, get]
   least: [113bn, 900m, Â£125m, Â£59m, Â£160m, Â£197bn, pariba, propel, 75bn, Â£57bn]

3) What is the average frequency of non-zero vector entries in any document?
   44.2

%%%%%%%%%%
%% Step 1
%%%%%%%%%%

1) Can you categorize the topic for each cluster? (list, comma separated)
   blunkett told alan inquiring secretary (Sorry! really don't know what it is)
   world film award
   seychel group
   winner return
   december month sale

2) What factors make clustering difficult?
   The norm of vectors are different, which means they have different length. This will
   give vector different weight in clustering.

3) Will we get better results with a lucky initial guess for cluster centers?
   (yes/no and a short explanation of why)
   No. Because the error is huge that has great influence in topic categorization. Even
   if we start with a lucky initial, the vectors of "correct result" may not have enough
   weight(small norm) so that kmeans may still lead us to the "wrong result".

%%%%%%%%%%
%% Step 2
%%%%%%%%%%

1) What problem from step 1 is solved now?
   It uses normalization to eliminates the error caused by different data vector length
   and treats every input as a probability distribution over terms.

2) What are the topics for clusters?
   sunderland join weird match group
   film award
   final group match winner
   sunderland join club
   sunderland match firm

3) Is the result better or worse than step 1? (give a short explanation as well)
   Yes. Firstly, the error becomes significantly smaller than step 1. In addition,
   it is easier to categorize the topic.

%%%%%%%%%%
%% Step 3
%%%%%%%%%%

1) What are the topics for clusters?
   government election
   best film award
   win final play film
   england, ireland, wale
   firm company market service

2) Why is the clustering better now?
   Because it handles the case of the word co-occurrence probabilities and consider
   words' correlation.

3) What is the general lesson you learned in clustering sparse, high-dimensional
   data?
   We should normalize the data and process them under unit length (treat them as a probability
   distributions). In addition, we should consider the correlation between dimensions and
   allow diffusion to cover the case of co-occurrence probabilities


%%%%%%%%%%
%% Step 5
%%%%%%%%%%

1) What is the total error difference between K-Means++ and random center initialization?
   0.3065400530109006

2) What is the mean and variance of total errors after running K-Means++ 5 times?
   Average: 3.3110697408699807
   Variance: 0.029025379197775352

3) Do the training errors appear to be more consistent?
   Yes

4) Do the topics appear to be more meaningful?
   Yes

%%%%%%%%%%
%%  K-Means vs GMM
%%%%%%%%%%

1) Under what scenarios do the methods find drastically different clusters? Why?
   When points in a cluster have some correlation with other cluster(e.g cross or one cluster covering on another),
   then GMM will give different results. This is because GMM is soft clustering. It solves the distribution parameters by
   calculating the maximum likelihood function and gives the probability of each data point generated
   by each Gaussian component while K-means directly gives which category an observation point
   belongs to, which is hard clustering. At the same time, GMM uses covariance in the calculation
   and applies the mutual constraints between different dimensions.

2) What happens to GMM as we increase the dimensionality of input feature? Does K-Means suffer from the same problem?
   There will be a high computational expense and take a lot of times. It is also prone to the problem that some
   covariance matrices are singular matrices, especially when several dimensions have strong correlations.
   This will not happen in K-Means since it does not consider correlation.

